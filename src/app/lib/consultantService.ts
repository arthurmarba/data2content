/**
 * @fileoverview Servi√ßo principal para obter respostas do consultor Tuca.
 * Vers√£o otimizada para buscar m√©tricas proativamente e envi√°-las no contexto inicial do LLM.
 * Timeout de leitura do stream aumentado.
 * @version 4.3.1
 */

import { logger } from '@/app/lib/logger';
import { normalizeText } from './intentService';
// Importa a fun√ß√£o adaptada do orquestrador
import { askLLMWithEnrichedContext } from './aiOrchestrator';
import * as stateService from '@/app/lib/stateService';
import * as dataService from './dataService'; // Garanta que getLatestAggregatedReport est√° aqui
import { UserNotFoundError } from '@/app/lib/errors';
import { getRandomGreeting } from './intentService';
import { IUser } from '@/app/models/User'; // Assumindo que IUser est√° em models
import { ChatCompletionMessageParam } from 'openai/resources/chat/completions'; // Ajuste se usar outro provedor
import { AggregatedReport } from './reportHelpers'; // Garanta que o tipo est√° correto

// Configura√ß√µes
const CACHE_TTL_SECONDS = Number(process.env.CACHE_TTL_SECONDS) || 60 * 5; // 5 minutos

// *** TIMEOUT AJUSTADO ***
// Aumenta o timeout para dar mais tempo para o LLM gerar respostas complexas ap√≥s function calls.
// Exemplo: 90 segundos (ajuste conforme necess√°rio)
const STREAM_READ_TIMEOUT_MS = Number(process.env.STREAM_READ_TIMEOUT_MS) || 90_000; // Aumentado de 50s para 90s (ou via env var)


/**
 * @interface EnrichedContext
 * @description Define a estrutura do contexto enviado ao LLM, incluindo dados do usu√°rio e m√©tricas.
 */
interface EnrichedContext {
    user: IUser;
    historyMessages: ChatCompletionMessageParam[];
    dialogueState?: stateService.DialogueState; // Estado da conversa (ex: √∫ltimo t√≥pico)
    latestReport?: AggregatedReport | null; // √öltimo relat√≥rio agregado
}

/**
 * Obt√©m a resposta do consultor Tuca para uma mensagem recebida.
 * Busca proativamente o √∫ltimo relat√≥rio de m√©tricas do usu√°rio.
 * @param {string} fromPhone N√∫mero de telefone do remetente.
 * @param {string} incoming Mensagem recebida do usu√°rio.
 * @returns {Promise<string>} A resposta do consultor.
 */
export async function getConsultantResponse(
    fromPhone: string,
    incoming: string
): Promise<string> {
    // Define uma TAG mais espec√≠fica para esta vers√£o
    const TAG = '[consultantService 4.3.1]';
    const start = Date.now();
    logger.info(`${TAG} ‚á¢ ${fromPhone.slice(-4)}‚Ä¶ ¬´${incoming.slice(0, 40)}¬ª`);

    // --- 0. Normaliza√ß√£o e Cache da Resposta ---
    const norm = normalizeText(incoming.trim());
    if (!norm) {
        logger.warn(`${TAG} Mensagem normalizada vazia.`);
        return `${getRandomGreeting('')} Pode repetir, por favor? N√£o entendi bem.`;
    }

    const cacheKey = `resp:${fromPhone}:${norm.slice(0, 100)}`;
    try {
        const cached = await stateService.getFromCache(cacheKey);
        if (cached) {
            logger.info(`${TAG} (cache hit) ${Date.now() - start} ms`);
            return cached; // Retorna resposta do cache
        }
        logger.info(`${TAG} (cache miss)`);
    } catch (cacheError) {
        logger.error(`${TAG} Erro ao buscar do cache Redis:`, cacheError);
        // Continua mesmo com erro no cache
    }

    // --- 1. Carregar Dados do Usu√°rio ---
    let user: IUser;
    try {
        user = await dataService.lookupUser(fromPhone);
        logger.debug(`${TAG} Usu√°rio ${user._id} carregado.`);
    } catch (e) {
        logger.error(`${TAG} Erro em lookupUser:`, e);
        if (e instanceof UserNotFoundError) {
            // Mant√©m a mensagem amig√°vel para novos usu√°rios
            return 'Ol√°! Parece que √© nosso primeiro contato por aqui. Para come√ßar, preciso fazer seu cadastro r√°pido. Pode me confirmar seu nome completo, por favor?';
        }
        // Outros erros podem ser problemas tempor√°rios
        return 'Tive um problema ao buscar seus dados. Poderia tentar novamente em alguns instantes? Se persistir, fale com o suporte. üôè';
    }
    const uid = user._id.toString();

    // --- 2. Carregar Contexto da Conversa (Estado e Hist√≥rico) ---
    let dialogueState: stateService.DialogueState = {};
    let historyString: string = '';
    try {
        // Busca estado e hist√≥rico em paralelo
        [dialogueState, historyString] = await Promise.all([
            stateService.getDialogueState(uid),
            stateService.getConversationHistory(uid),
        ]);
        logger.debug(`${TAG} Estado carregado. Hist√≥rico tem ${historyString.length} chars.`);
    } catch (stateError) {
        logger.error(`${TAG} Erro ao buscar estado/hist√≥rico do Redis:`, stateError);
        // N√£o cr√≠tico, continua sem estado/hist√≥rico se falhar
    }

    // --- 3. Preparar Hist√≥rico de Mensagens para o LLM ---
    const historyMessages: ChatCompletionMessageParam[] = [];
    if (historyString) {
        try {
            const lines = historyString.trim().split('\n');
            let currentRole: 'user' | 'assistant' | null = null;
            let currentContent = '';
            for (const line of lines) {
                if (line.startsWith('User: ')) {
                    if (currentRole) historyMessages.push({ role: currentRole, content: currentContent.trim() });
                    currentRole = 'user';
                    currentContent = line.substring(6);
                } else if (line.startsWith('Assistant: ')) {
                    if (currentRole) historyMessages.push({ role: currentRole, content: currentContent.trim() });
                    currentRole = 'assistant';
                    currentContent = line.substring(11);
                } else if (currentRole) {
                    // Continua o conte√∫do da mensagem anterior se n√£o for um novo prefixo
                    currentContent += '\n' + line;
                }
            }
            // Adiciona a √∫ltima mensagem pendente
            if (currentRole) historyMessages.push({ role: currentRole, content: currentContent.trim() });
            logger.debug(`${TAG} Hist√≥rico convertido para ${historyMessages.length} mensagens.`);
        } catch (parseError) {
            logger.error(`${TAG} Erro ao parsear hist√≥rico:`, parseError);
            // Limpa o hist√≥rico em caso de erro de parse para n√£o enviar dados inv√°lidos
            historyMessages.length = 0;
        }
    }

    // --- 4. Carregar Dados de M√©tricas (Relat√≥rio Agregado) ---
    let latestReport: AggregatedReport | null = null;
    try {
        // Chama a nova fun√ß√£o em dataService
        latestReport = await dataService.getLatestAggregatedReport(uid);
        if (latestReport) {
            // Log apenas se o relat√≥rio for encontrado e tiver dados
            logger.debug(`${TAG} Relat√≥rio agregado carregado (Overall Stats: ${!!latestReport.overallStats}).`);
        } else {
            logger.info(`${TAG} Nenhum relat√≥rio agregado recente encontrado para o usu√°rio ${uid}.`);
        }
    } catch (reportError) {
        logger.error(`${TAG} Erro ao buscar relat√≥rio agregado:`, reportError);
        // N√£o cr√≠tico, o LLM ser√° instru√≠do a lidar com a aus√™ncia de dados
    }

    // --- 5. Preparar Contexto Enriquecido para o LLM ---
    const enrichedContext: EnrichedContext = {
        user: user, // Dados do usu√°rio (nome, etc.)
        historyMessages: historyMessages, // Hist√≥rico da conversa formatado
        dialogueState: dialogueState, // Estado atual da conversa
        latestReport: latestReport, // √öltimo relat√≥rio (pode ser null)
    };

    // --- 6. Chamar o LLM com o Contexto Enriquecido e Processar Resposta ---
    let finalText = '';
    let historyFromLLM: ChatCompletionMessageParam[] = []; // Armazena o hist√≥rico retornado pelo LLM
    let readCounter = 0; // Contador de chunks lidos do stream
    let streamTimeout: NodeJS.Timeout | null = null;
    let reader: ReadableStreamDefaultReader<string> | null = null;

    try {
        logger.debug(`${TAG} Chamando askLLMWithEnrichedContext...`);
        // Chama a fun√ß√£o adaptada no orquestrador
        const { stream, history: updatedHistory } = await askLLMWithEnrichedContext(
            enrichedContext, // Passa todo o contexto
            incoming // Passa a mensagem atual do usu√°rio
        );
        // Guarda o hist√≥rico retornado (pode conter chamadas de fun√ß√£o e a resposta final)
        historyFromLLM = updatedHistory;
        logger.debug(`${TAG} askLLMWithEnrichedContext retornou. Iniciando leitura do stream...`);

        reader = stream.getReader();

        // Define um timeout para a leitura do stream (AGORA MAIOR)
        streamTimeout = setTimeout(() => {
            logger.warn(`${TAG} Timeout (${STREAM_READ_TIMEOUT_MS}ms) durante leitura do stream. Cancelando reader.`);
            streamTimeout = null; // Evita clear duplo no finally
            reader?.cancel(`Stream reading timeout after ${STREAM_READ_TIMEOUT_MS}ms`)
                  .catch(e => logger.error(`${TAG} Erro ao cancelar reader no timeout:`, e));
        }, STREAM_READ_TIMEOUT_MS);

        // Loop para ler os chunks do stream
        while (true) {
            readCounter++;
            let value: string | undefined;
            let done: boolean | undefined;

            try {
                const result = await reader.read();
                // Se o timeout j√° ocorreu, ignora o resultado tardio
                if (streamTimeout === null && !result.done) {
                    logger.warn(`${TAG} [Leitura ${readCounter}] Leitura retornou ap√≥s timeout/cancelamento, ignorando chunk.`);
                    continue; // Ignora este chunk e tenta ler o pr√≥ximo (ou finalizar)
                }
                value = result.value;
                done = result.done;

            } catch (readError: any) {
                // Erros comuns aqui incluem cancelamento pelo timeout
                logger.error(`${TAG} [Leitura ${readCounter}] Erro em reader.read(): ${readError.message}`);
                if (streamTimeout) clearTimeout(streamTimeout);
                streamTimeout = null;
                // Considera erro fatal para a leitura
                throw new Error(`Erro ao ler stream: ${readError.message}`);
            }

            if (done) {
                logger.debug(`${TAG} [Leitura ${readCounter}] Stream finalizado (done=true).`);
                break; // Sai do loop while
            }

            if (typeof value === 'string') {
                finalText += value; // Acumula o texto da resposta
            } else {
                // Isso n√£o deveria acontecer se done √© false, loga como aviso
                logger.warn(`${TAG} [Leitura ${readCounter}] Recebido 'value' undefined, mas 'done' √© false.`);
            }
        }

        // Limpa o timeout se o stream terminou antes do tempo limite
        if (streamTimeout) {
            clearTimeout(streamTimeout);
            logger.debug(`${TAG} Leitura do stream conclu√≠da antes do timeout.`);
        }

        logger.debug(`${TAG} Texto final montado: ${finalText.length} chars.`);

        // Verifica se a resposta final n√£o est√° vazia
        if (finalText.trim().length === 0) {
            logger.warn(`${TAG} Stream finalizado mas finalText est√° vazio ap√≥s ${readCounter} leituras.`);
            // Pode ser um problema no LLM ou na fun√ß√£o chamada
            return 'Hum... n√£o consegui gerar uma resposta completa agora. Pode tentar de novo?';
        }

    } catch (err: any) {
        logger.error(`${TAG} Erro durante chamada ao LLM ou leitura do stream (ap√≥s ${readCounter} leituras):`, err);
        // Garante que o timeout seja limpo em caso de erro
        if (streamTimeout) clearTimeout(streamTimeout);
        streamTimeout = null;
        // Retorna mensagem de erro gen√©rica para o usu√°rio
        return 'Ops! Tive uma dificuldade t√©cnica aqui. Poderia tentar sua pergunta novamente em alguns instantes? üôè';
    } finally {
        // Garante a libera√ß√£o do lock do reader
        if (reader) {
            try {
                await reader.releaseLock();
            } catch (releaseError) {
                // Loga o erro mas n√£o impede o fluxo
                logger.error(`${TAG} Erro (n√£o fatal) ao liberar reader lock no finally:`, releaseError);
            }
        }
    }

    // --- 7. Persist√™ncia P√≥s-Resposta ---
    try {
        logger.debug(`${TAG} Iniciando persist√™ncia no Redis...`);
        // Idealmente, o orchestrator deveria retornar o estado atualizado ap√≥s chamadas de fun√ß√£o.
        // Se n√£o retornar, usamos o estado que t√≠nhamos antes da chamada.
        const nextState = { ...(enrichedContext.dialogueState || {}), lastInteraction: Date.now() };

        // Usa o hist√≥rico retornado pelo orchestrator, que inclui a intera√ß√£o completa
        const newHistoryString = historyFromLLM
            .filter(msg => msg.role === 'user' || (msg.role === 'assistant' && typeof msg.content === 'string' && msg.content.trim().length > 0)) // Garante que assistant content existe e n√£o √© vazio
            .map(msg => {
                // Formata corretamente, tratando poss√≠veis conte√∫dos n√£o-string (embora filtrados)
                const rolePrefix = msg.role === 'user' ? 'User' : 'Assistant';
                const content = typeof msg.content === 'string' ? msg.content : JSON.stringify(msg.content); // Fallback para JSON se n√£o for string
                return `${rolePrefix}: ${content}`;
            })
            .join('\n');

        // Executa as opera√ß√µes de persist√™ncia em paralelo (sem esperar todas)
        await Promise.allSettled([
            stateService.updateDialogueState(uid, nextState),
            stateService.setConversationHistory(uid, newHistoryString),
            stateService.setInCache(cacheKey, finalText, CACHE_TTL_SECONDS),
            stateService.incrementUsageCounter(uid), // Incrementa contador de uso
        ]);
        logger.debug(`${TAG} Persist√™ncia no Redis conclu√≠da.`);
    } catch (persistError) {
        logger.error(`${TAG} Erro durante a persist√™ncia no Redis (n√£o fatal):`, persistError);
        // Loga o erro mas retorna a resposta ao usu√°rio mesmo assim
    }

    // --- Finaliza√ß√£o ---
    const duration = Date.now() - start;
    logger.info(`${TAG} ‚úì ok ${duration} ms. Retornando ${finalText.length} chars.`);
    return finalText; // Retorna a resposta final para o usu√°rio
}


// ----- Fun√ß√£o de Resumo Semanal (Opcional) -----
// Mantida como exemplo, mas idealmente usaria o mesmo fluxo do Tuca para an√°lises.

/**
 * Gera um resumo estrat√©gico semanal baseado em um relat√≥rio agregado.
 * @param {string} userName Nome do usu√°rio.
 * @param {AggregatedReport} report O relat√≥rio agregado.
 * @returns {Promise<string>} O resumo gerado pela IA.
 */
export async function generateStrategicWeeklySummary(
  userName: string,
  report: AggregatedReport
): Promise<string> {
  const TAG = '[weeklySummary]';
  if (!report?.overallStats) {
      logger.warn(`${TAG} OverallStats ausente no relat√≥rio para ${userName}.`);
      return 'N√£o foi poss√≠vel gerar o resumo semanal: dados insuficientes no relat√≥rio.';
  }

  // Prompt simples para resumo direto das m√©tricas gerais.
  // Para an√°lises mais profundas, seria melhor chamar askLLMWithEnrichedContext.
  const PROMPT = `Como consultor estrat√©gico, resuma em 3 bullets concisos os principais destaques (positivos ou pontos de aten√ß√£o) das m√©tricas gerais de ${userName} desta semana, baseado nestes dados: ${JSON.stringify(
    report.overallStats
  )}. Foco em insights acion√°veis.`;

  try {
    // Usar uma chamada direta √† IA pode ser mais simples para um resumo b√°sico.
    // Se precisar de an√°lise complexa ou acesso a outras fun√ß√µes, use o fluxo principal.
    // Substitua 'callOpenAIForQuestion' pela sua fun√ß√£o real de chamada direta √† IA.
    // return await callOpenAIForQuestion(PROMPT); // Descomente e ajuste se necess√°rio
    logger.warn(`${TAG} Chamada direta √† IA para resumo n√£o implementada/configurada.`);
    return `Resumo semanal para ${userName} (simulado): [Insight 1 baseado em ${Object.keys(report.overallStats)[0]}], [Insight 2 baseado em ${Object.keys(report.overallStats)[1]}], [Insight 3 baseado em ${Object.keys(report.overallStats)[2]}]`; // Placeholder
  } catch (e) {
    logger.error(`${TAG} Erro ao gerar resumo semanal para ${userName}:`, e);
    return `N√£o consegui gerar o resumo semanal para ${userName} agora devido a um erro.`;
  }
}
